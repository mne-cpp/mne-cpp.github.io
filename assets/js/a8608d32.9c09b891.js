"use strict";(globalThis.webpackChunkmne_cpp_website=globalThis.webpackChunkmne_cpp_website||[]).push([[2629],{96625(e,r,n){n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"development/api-disp3d","title":"Disp3D (RHI)","description":"Starting with MNE-CPP v2.0, the 3D visualization library has been rebuilt on top of Qt\'s Rendering Hardware Interface (RHI). The library is now called disp3D_rhi (namespace DISP3DRHILIB). Unlike the previous Qt3D-based implementation, the RHI backend talks directly to the platform\'s native graphics API \u2014 Metal on macOS, Vulkan on Linux, Direct3D 11/12 on Windows, or OpenGL as a fallback \u2014 giving MNE-CPP a thinner, more portable rendering stack.","source":"@site/docs/development/api-disp3d.mdx","sourceDirName":"development","slug":"/development/api-disp3d","permalink":"/docs/development/api-disp3d","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Disp3D (RHI)","sidebar_label":"Disp3D (RHI)"},"sidebar":"developmentSidebar","previous":{"title":"Library API","permalink":"/docs/development/api"},"next":{"title":"Connectivity","permalink":"/docs/development/api-connectivity"}}');var s=n(74848),t=n(28453);const d={title:"Disp3D (RHI)",sidebar_label:"Disp3D (RHI)"},a="Disp3D \u2014 RHI-Based 3D Visualization",c={},l=[{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Key Differences from the Qt3D Implementation",id:"key-differences-from-the-qt3d-implementation",level:3},{value:"Module Breakdown",id:"module-breakdown",level:2},{value:"View Layer",id:"view-layer",level:3},{value:"Render Engine",id:"render-engine",level:3},{value:"Renderable Objects",id:"renderable-objects",level:3},{value:"Data Model",id:"data-model",level:3},{value:"Core",id:"core",level:3},{value:"Input",id:"input",level:3},{value:"Scene Managers",id:"scene-managers",level:3},{value:"Background Workers",id:"background-workers",level:3},{value:"Usage",id:"usage",level:2},{value:"Quick Start \u2014 Visualizing a Brain Surface",id:"quick-start--visualizing-a-brain-surface",level:3},{value:"Adding a Connectivity Network",id:"adding-a-connectivity-network",level:3},{value:"Shader Modes and Visualization Modes",id:"shader-modes-and-visualization-modes",level:2}];function o(e){const r={admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"disp3d--rhi-based-3d-visualization",children:"Disp3D \u2014 RHI-Based 3D Visualization"})}),"\n",(0,s.jsxs)(r.p,{children:["Starting with MNE-CPP v2.0, the 3D visualization library has been rebuilt on top of Qt's ",(0,s.jsx)(r.strong,{children:"Rendering Hardware Interface (RHI)"}),". The library is now called ",(0,s.jsx)(r.code,{children:"disp3D_rhi"})," (namespace ",(0,s.jsx)(r.code,{children:"DISP3DRHILIB"}),"). Unlike the previous Qt3D-based implementation, the RHI backend talks directly to the platform's native graphics API \u2014 Metal on macOS, Vulkan on Linux, Direct3D 11/12 on Windows, or OpenGL as a fallback \u2014 giving MNE-CPP a thinner, more portable rendering stack."]}),"\n",(0,s.jsx)(r.admonition,{title:"Why RHI?",type:"info",children:(0,s.jsx)(r.p,{children:"Qt3D provided a high-level scene-graph but added significant abstraction overhead. With RHI, the library manages its own render pipeline, vertex buffers, and shader programs directly, resulting in lower latency, smaller binary size, and better control over GPU resource lifetime \u2014 all critical for real-time neuroimaging visualization."})}),"\n",(0,s.jsx)(r.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(r.p,{children:"The library follows a clean separation of concerns inspired by several well-known software patterns:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Model\u2013View separation"})," \u2014 The ",(0,s.jsx)(r.code,{children:"BrainTreeModel"})," (a ",(0,s.jsx)(r.code,{children:"QStandardItemModel"}),") owns all scene data and metadata. ",(0,s.jsx)(r.code,{children:"BrainView"})," reads the model to decide ",(0,s.jsx)(r.em,{children:"what"})," to show; ",(0,s.jsx)(r.code,{children:"BrainRenderer"})," decides ",(0,s.jsx)(r.em,{children:"how"})," to draw it. Neither the model nor the renderer know about each other directly \u2014 ",(0,s.jsx)(r.code,{children:"BrainView"})," acts as the mediator."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Retained-mode rendering"})," \u2014 Renderable objects (",(0,s.jsx)(r.code,{children:"BrainSurface"}),", ",(0,s.jsx)(r.code,{children:"DipoleObject"}),", ",(0,s.jsx)(r.code,{children:"NetworkObject"}),", ",(0,s.jsx)(r.code,{children:"SourceEstimateOverlay"}),") each own their GPU resources (vertex buffers, index buffers, instance buffers, uniform buffers). They create these resources once on initialization and only update the data that changes per frame (e.g., vertex colors for a new time sample). This avoids per-frame allocation and keeps draw calls cheap."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Strategy pattern for shaders"})," \u2014 The ",(0,s.jsx)(r.code,{children:"BrainRenderer"})," maintains a set of pre-compiled shader pipelines (",(0,s.jsx)(r.code,{children:"Standard"}),", ",(0,s.jsx)(r.code,{children:"Holographic"}),", ",(0,s.jsx)(r.code,{children:"Anatomical"}),", ",(0,s.jsx)(r.code,{children:"Dipole"}),", ",(0,s.jsx)(r.code,{children:"XRay"}),", ",(0,s.jsx)(r.code,{children:"ShowNormals"}),"). Switching visual style is a pipeline swap, not a recompilation."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Observer / worker pattern"})," \u2014 Real-time data (sensor streams, source estimates) flows through dedicated ",(0,s.jsx)(r.code,{children:"QObject"}),"-based workers running on background threads. Workers emit signals carrying per-vertex color arrays, which ",(0,s.jsx)(r.code,{children:"BrainView"})," applies to the appropriate renderable before the next draw call. This keeps the render thread free from heavy computation."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Composition over inheritance"})," \u2014 Rather than deep inheritance hierarchies (as in the old Qt3D design where every renderable was a ",(0,s.jsx)(r.code,{children:"QEntity"})," subclass), the RHI design composes independent objects: a ",(0,s.jsx)(r.code,{children:"BrainSurface"})," is ",(0,s.jsx)(r.em,{children:"not"})," a widget or an entity \u2014 it is a plain C++ object that knows how to upload geometry and draw itself when asked."]}),"\n"]}),"\n",(0,s.jsx)(r.mermaid,{value:"classDiagram\n    class ViewLayer {\n        BrainView\n        MultiViewLayout\n    }\n\n    class RenderEngine {\n        BrainRenderer\n        GLSL Shaders\n    }\n\n    class RenderableObjects {\n        BrainSurface\n        DipoleObject\n        NetworkObject\n        SourceEstimateOverlay\n    }\n\n    class DataModel {\n        BrainTreeModel\n        Surface / BEM / Digitizer\n        Dipole / Network / SourceSpace\n    }\n\n    class Input {\n        CameraController\n        RayPicker\n    }\n\n    class Core {\n        RenderTypes\n        ViewState\n        DataLoader\n    }\n\n    class SceneManagers {\n        SensorFieldMapper\n        SourceEstimateManager\n        RtSensorStreamManager\n    }\n\n    class BackgroundWorkers {\n        RtSensorDataWorker\n        RtSensorInterpolationMatWorker\n        RtSourceDataWorker\n        StcLoadingWorker\n    }\n\n    ViewLayer --\x3e RenderEngine : drives\n    RenderEngine --\x3e RenderableObjects : draws\n    ViewLayer --\x3e DataModel : reads\n    ViewLayer --\x3e Input : delegates\n    ViewLayer --\x3e SceneManagers : coordinates\n    SceneManagers --\x3e BackgroundWorkers : dispatches"}),"\n",(0,s.jsx)(r.h3,{id:"key-differences-from-the-qt3d-implementation",children:"Key Differences from the Qt3D Implementation"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Aspect"}),(0,s.jsx)(r.th,{children:"Qt3D (v1.x)"}),(0,s.jsx)(r.th,{children:"RHI (v2.0)"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Widget base"})}),(0,s.jsxs)(r.td,{children:["Custom ",(0,s.jsx)(r.code,{children:"QWidget"})," + ",(0,s.jsx)(r.code,{children:"QSurface3DFormat"})]}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"QRhiWidget"})})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Render control"})}),(0,s.jsx)(r.td,{children:"Qt3D framegraph / scenegraph"}),(0,s.jsxs)(r.td,{children:["Direct ",(0,s.jsx)(r.code,{children:"QRhi"})," pipeline + command buffer"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Shader language"})}),(0,s.jsx)(r.td,{children:"GLSL via Qt3D materials"}),(0,s.jsx)(r.td,{children:"GLSL compiled per-backend by Qt RHI"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Data model"})}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"Data3DTreeModel"})," + ",(0,s.jsx)(r.code,{children:"Renderable3DEntity"})," \u2192 ",(0,s.jsx)(r.code,{children:"QEntity"})]}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"BrainTreeModel"})," + ",(0,s.jsx)(r.code,{children:"BrainSurface"})," / ",(0,s.jsx)(r.code,{children:"DipoleObject"})," / ",(0,s.jsx)(r.code,{children:"NetworkObject"})]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Multi-view"})}),(0,s.jsx)(r.td,{children:"Not built-in"}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"MultiViewLayout"})," with 1\u20134 panes"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Instanced rendering"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"GeometryMultiplier"})}),(0,s.jsx)(r.td,{children:"Native QRhi instance buffers"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Vertex picking"})}),(0,s.jsx)(r.td,{children:"Not implemented"}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"RayPicker"})," with CPU ray\u2013triangle intersection"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.strong,{children:"Graphics backends"})}),(0,s.jsx)(r.td,{children:"OpenGL only"}),(0,s.jsx)(r.td,{children:"Metal, Vulkan, D3D 11/12, OpenGL"})]})]})]}),"\n",(0,s.jsx)(r.h2,{id:"module-breakdown",children:"Module Breakdown"}),"\n",(0,s.jsx)(r.h3,{id:"view-layer",children:"View Layer"}),"\n",(0,s.jsxs)(r.p,{children:["The view layer implements the ",(0,s.jsx)(r.strong,{children:"Mediator pattern"})," \u2014 ",(0,s.jsx)(r.code,{children:"BrainView"})," sits between user input, the data model, and the renderer, coordinating their interactions without letting them reference each other directly."]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"BrainView"})})," \u2014 The top-level widget, inheriting from ",(0,s.jsx)(r.code,{children:"QRhiWidget"})," (Qt's portable RHI surface widget). On each frame, Qt calls ",(0,s.jsx)(r.code,{children:"BrainView::render()"}),", which assembles a ",(0,s.jsx)(r.code,{children:"SceneData"})," struct containing the current MVP matrix, camera position, light direction, and overlay mode, then hands it to ",(0,s.jsx)(r.code,{children:"BrainRenderer"})," for drawing. ",(0,s.jsx)(r.code,{children:"BrainView"})," also handles all mouse/keyboard events and dispatches them to the ",(0,s.jsx)(r.code,{children:"CameraController"})," (for orbit/zoom/pan) or ",(0,s.jsx)(r.code,{children:"RayPicker"})," (for vertex selection). It supports two modes: ",(0,s.jsx)(r.code,{children:"SingleView"})," with an interactive orbiting camera, and ",(0,s.jsx)(r.code,{children:"MultiView"})," with up to four fixed-angle viewports (e.g., top, left, front, free)."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"MultiViewLayout"})})," \u2014 A pure-geometry helper with no widget dependencies. It stores fractional split positions and computes viewport rectangles, splitter hit-testing, and minimum pane sizes. ",(0,s.jsx)(r.code,{children:"BrainView"})," queries this class for the pixel rectangles of each pane before issuing scissored draw calls. The layout supports 1, 2, 3, or 4 panes with draggable separators."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"render-engine",children:"Render Engine"}),"\n",(0,s.jsxs)(r.p,{children:["The render engine is where the ",(0,s.jsx)(r.strong,{children:"Strategy pattern"})," is most visible. ",(0,s.jsx)(r.code,{children:"BrainRenderer"})," holds a map of pre-compiled ",(0,s.jsx)(r.code,{children:"QRhiGraphicsPipeline"})," objects \u2014 one per ",(0,s.jsx)(r.code,{children:"ShaderMode"})," \u2014 and selects the appropriate pipeline at draw time."]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"BrainRenderer"})})," \u2014 The low-level workhorse. It creates and caches ",(0,s.jsx)(r.code,{children:"QRhiGraphicsPipeline"})," objects, manages shared uniform buffers (for the scene-wide MVP and lighting data), and iterates over all visible renderables to issue draw calls. Each renderable provides its own vertex/index/instance buffers; ",(0,s.jsx)(r.code,{children:"BrainRenderer"})," simply binds them and calls ",(0,s.jsx)(r.code,{children:"drawIndexed()"})," or ",(0,s.jsx)(r.code,{children:"draw()"}),". The renderer is stateless between frames \u2014 all per-frame data arrives via the ",(0,s.jsx)(r.code,{children:"SceneData"})," struct, making it easy to render the same scene from multiple camera angles (multi-view)."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"GLSL Shaders"})," \u2014 Six shader programs, each consisting of a ",(0,s.jsx)(r.code,{children:".vert"})," and ",(0,s.jsx)(r.code,{children:".frag"})," file. Qt's RHI layer compiles these at load time into the native shading language of the active backend (Metal Shading Language, SPIR-V for Vulkan, HLSL for D3D, or GLSL for OpenGL). The shaders receive per-vertex attributes (position, normal, packed ABGR color, packed ABGR annotation color) and a uniform block with the scene data. The ",(0,s.jsx)(r.code,{children:"overlayMode"})," uniform controls which color channel the fragment shader samples \u2014 surface, annotation, scientific colormap, or source estimate."]}),"\n"]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Shader"}),(0,s.jsx)(r.th,{children:"Visual Style"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"standard"})}),(0,s.jsx)(r.td,{children:"Phong lighting (default)"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"holographic"})}),(0,s.jsx)(r.td,{children:"Two-sided holographic effect"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"anatomical"})}),(0,s.jsx)(r.td,{children:"Curvature-based coloring"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"dipole"})}),(0,s.jsx)(r.td,{children:"Dipole cone rendering"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"xray"})}),(0,s.jsx)(r.td,{children:"Semi-transparent x-ray"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"shownormals"})}),(0,s.jsx)(r.td,{children:"Surface-normal visualization"})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"renderable-objects",children:"Renderable Objects"}),"\n",(0,s.jsxs)(r.p,{children:["Renderables follow the ",(0,s.jsx)(r.strong,{children:"Self-Contained Resource"})," pattern \u2014 each object manages the full lifecycle of its GPU resources (creation, update, destruction) and exposes a uniform interface (",(0,s.jsx)(r.code,{children:"initResources()"}),", ",(0,s.jsx)(r.code,{children:"updateResources()"}),", ",(0,s.jsx)(r.code,{children:"draw()"}),") that ",(0,s.jsx)(r.code,{children:"BrainRenderer"})," calls. This keeps the renderer agnostic about data types."]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Class"}),(0,s.jsx)(r.th,{children:"Purpose"}),(0,s.jsx)(r.th,{children:"GPU Resources"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"BrainSurface"})}),(0,s.jsx)(r.td,{children:"Cortical or BEM mesh with per-vertex curvature, annotation, and STC color channels"}),(0,s.jsxs)(r.td,{children:["Vertex buffer (",(0,s.jsx)(r.code,{children:"VertexData"}),": pos, normal, ABGR base, ABGR annotation), index buffer, uniform buffer"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"DipoleObject"})}),(0,s.jsx)(r.td,{children:"Equivalent current dipoles rendered as instanced cones pointing in the moment direction"}),(0,s.jsx)(r.td,{children:"Shared cone geometry + per-instance transform/color buffer"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"NetworkObject"})}),(0,s.jsx)(r.td,{children:"Connectivity graph \u2014 nodes as instanced spheres (diameter \u221d degree), edges as instanced cylinders (diameter and color \u221d weight)"}),(0,s.jsx)(r.td,{children:"Shared sphere/cylinder geometry + per-instance transform/color buffers"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"SourceEstimateOverlay"})}),(0,s.jsxs)(r.td,{children:["Per-vertex STC amplitude overlay streamed onto a ",(0,s.jsx)(r.code,{children:"BrainSurface"})]}),(0,s.jsx)(r.td,{children:"Updates the host surface's vertex color channel in-place"})]})]})]}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"VertexData"})," struct deserves special mention. It packs four attributes into a compact interleaved layout:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-cpp",children:"struct VertexData {\n    QVector3D pos;              // vertex position\n    QVector3D norm;             // vertex normal\n    uint32_t  color;            // curvature / base color   (ABGR packed)\n    uint32_t  colorAnnotation;  // annotation region color  (ABGR packed)\n};\n"})}),"\n",(0,s.jsxs)(r.p,{children:["Colors are stored as packed ",(0,s.jsx)(r.code,{children:"uint32_t"})," values (",(0,s.jsx)(r.code,{children:"packABGR()"}),") rather than ",(0,s.jsx)(r.code,{children:"QVector4D"})," floats, halving the per-vertex color footprint \u2014 a meaningful saving for high-resolution cortical meshes with 160 000+ vertices."]}),"\n",(0,s.jsx)(r.h3,{id:"data-model",children:"Data Model"}),"\n",(0,s.jsxs)(r.p,{children:["The data model applies the ",(0,s.jsx)(r.strong,{children:"Composite pattern"})," via Qt's ",(0,s.jsx)(r.code,{children:"QStandardItemModel"})," tree structure. Every scene object is represented as a tree item; parent items group children logically (subject \u2192 hemisphere \u2192 surface, or subject \u2192 measurement \u2192 network). The tree can be displayed in a ",(0,s.jsx)(r.code,{children:"QTreeView"})," to give users interactive visibility toggles, color pickers, and threshold sliders without any custom widget work."]}),"\n",(0,s.jsxs)(r.p,{children:["Crucially, tree items in the RHI design are ",(0,s.jsx)(r.strong,{children:"not"})," renderables. In the old Qt3D design, ",(0,s.jsx)(r.code,{children:"Renderable3DEntity"})," ",(0,s.jsx)(r.em,{children:"was"})," a ",(0,s.jsx)(r.code,{children:"QEntity"})," \u2014 the data model and the scenegraph were one and the same. In the RHI design, tree items are pure data/metadata containers. ",(0,s.jsx)(r.code,{children:"BrainView"})," reads the model to decide which ",(0,s.jsx)(r.code,{children:"BrainSurface"}),", ",(0,s.jsx)(r.code,{children:"DipoleObject"}),", or ",(0,s.jsx)(r.code,{children:"NetworkObject"})," instances should be visible, then passes them to ",(0,s.jsx)(r.code,{children:"BrainRenderer"}),". This separation makes it straightforward to render the same data in multiple viewports with different visibility profiles."]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Tree Item"}),(0,s.jsx)(r.th,{children:"Data Source"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"SurfaceTreeItem"})}),(0,s.jsx)(r.td,{children:"FreeSurfer surface + annotation"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"BemTreeItem"})}),(0,s.jsx)(r.td,{children:"BEM compartment surface"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"DigitizerTreeItem"})," / ",(0,s.jsx)(r.code,{children:"DigitizerSetTreeItem"})]}),(0,s.jsx)(r.td,{children:"HPI, fiducials, head-shape points"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"DipoleTreeItem"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ECDSet"})})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"NetworkTreeItem"})}),(0,s.jsxs)(r.td,{children:[(0,s.jsx)(r.code,{children:"Network"})," from the Connectivity library"]})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"SourceSpaceTreeItem"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"MNESourceSpace"})})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"core",children:"Core"}),"\n",(0,s.jsx)(r.p,{children:"The core module provides shared types and state that every other module depends on, without pulling in heavy Qt or QRhi headers."}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"RenderTypes"})})," \u2014 A lightweight header-only file defining ",(0,s.jsx)(r.code,{children:"ShaderMode"})," and ",(0,s.jsx)(r.code,{children:"VisualizationMode"})," enums and the ",(0,s.jsx)(r.code,{children:"packABGR()"})," utility. It deliberately avoids any QRhi includes, so public API headers can use these enums without leaking GPU implementation details to downstream consumers."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"ViewState"})})," \u2014 Captures all per-viewport settings: camera orientation, lighting parameters, and a ",(0,s.jsx)(r.code,{children:"ViewVisibilityProfile"})," struct with independent boolean toggles for each data layer (LH, RH, BEM head, BEM outer skull, BEM inner skull, MEG sensors, etc.). Each viewport in a multi-view layout carries its own ",(0,s.jsx)(r.code,{children:"ViewState"}),", enabling pane-specific views of the same dataset."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"DataLoader"})})," \u2014 Centralized utilities for loading FreeSurfer surfaces, annotations, and BEM data into the model and creating the corresponding renderables."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"input",children:"Input"}),"\n",(0,s.jsxs)(r.p,{children:["Input handling is decoupled from the widget, following the ",(0,s.jsx)(r.strong,{children:"Strategy pattern"})," \u2014 ",(0,s.jsx)(r.code,{children:"BrainView"})," delegates input interpretation to specialized objects rather than handling it in monolithic ",(0,s.jsx)(r.code,{children:"mousePressEvent"})," overrides."]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"CameraController"})})," \u2014 A widget-independent camera that takes mouse delta values and produces a ",(0,s.jsx)(r.code,{children:"CameraResult"})," struct (projection, view, and model matrices, plus camera position and up vector). It supports orbiting around a focus point, dolly zoom, and panning. Because it has no widget dependency, it can compute matrices for any viewport rectangle, which is how multi-view rendering works \u2014 ",(0,s.jsx)(r.code,{children:"BrainView"})," calls ",(0,s.jsx)(r.code,{children:"CameraController"})," once per pane with different viewport sizes."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"RayPicker"})})," \u2014 Performs CPU-side ray\u2013triangle intersection testing against all visible ",(0,s.jsx)(r.code,{children:"BrainSurface"})," meshes. Given a mouse position and viewport rectangle, it unprojects a ray and walks the triangle list of each surface to find the closest intersection. The result is a ",(0,s.jsx)(r.code,{children:"PickResult"})," carrying the hit point, surface key, vertex index, distance, and (if available) the FreeSurfer annotation region name and label ID. Dipole picking is also supported."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"scene-managers",children:"Scene Managers"}),"\n",(0,s.jsxs)(r.p,{children:["Scene managers encapsulate domain-specific logic that would otherwise bloat ",(0,s.jsx)(r.code,{children:"BrainView"}),". Each manager owns the lifecycle of its workers and the mapping between raw data and renderable updates."]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"SensorFieldMapper"})})," \u2014 Computes and caches the sparse interpolation matrix that maps sensor-level data (MEG channels or EEG electrodes) onto a scalp or helmet surface mesh. When new samples arrive, it multiplies the data vector by the interpolation matrix to produce per-vertex color values."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"SourceEstimateManager"})})," \u2014 Handles loading and streaming of MNE source estimates (STC files or real-time data). It manages the interpolation from source-space vertices to the high-resolution cortical surface mesh and drives the ",(0,s.jsx)(r.code,{children:"SourceEstimateOverlay"}),"."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"RtSensorStreamManager"})})," \u2014 Lifecycle manager for ",(0,s.jsx)(r.code,{children:"RtSensorDataController"}),". It encapsulates the start/stop/configure cycle of real-time sensor streaming, keeping ",(0,s.jsx)(r.code,{children:"BrainView"})," clean. Internally it manages ",(0,s.jsx)(r.code,{children:"RtSensorDataWorker"})," and ",(0,s.jsx)(r.code,{children:"RtSensorInterpolationMatWorker"})," on background threads."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"background-workers",children:"Background Workers"}),"\n",(0,s.jsxs)(r.p,{children:["Workers implement the ",(0,s.jsx)(r.strong,{children:"Producer\u2013Consumer pattern"})," on Qt's thread infrastructure. Each worker lives on a dedicated ",(0,s.jsx)(r.code,{children:"QThread"})," and communicates with the main/render thread via queued signal\u2013slot connections. This guarantees that heavy computation (interpolation matrix construction, per-vertex color mapping) never blocks the render loop."]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Worker"}),(0,s.jsx)(r.th,{children:"Role"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"RtSensorDataWorker"})}),(0,s.jsx)(r.td,{children:"Receives raw sensor samples, applies the interpolation matrix, and emits per-vertex color arrays ready for GPU upload"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"RtSensorInterpolationMatWorker"})}),(0,s.jsx)(r.td,{children:"Builds the sparse interpolation matrix mapping sensor positions to surface vertices (recomputed when sensor geometry changes)"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"RtSourceDataWorker"})}),(0,s.jsx)(r.td,{children:"Streams source-estimate samples and computes per-vertex STC colors"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"RtSourceInterpolationMatWorker"})}),(0,s.jsx)(r.td,{children:"Builds the source-space interpolation matrix"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"StcLoadingWorker"})}),(0,s.jsxs)(r.td,{children:["Asynchronous STC file I/O \u2014 loads large ",(0,s.jsx)(r.code,{children:".stc"})," files without blocking the UI"]})]})]})]}),"\n",(0,s.jsx)(r.h2,{id:"usage",children:"Usage"}),"\n",(0,s.jsx)(r.h3,{id:"quick-start--visualizing-a-brain-surface",children:"Quick Start \u2014 Visualizing a Brain Surface"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-cpp",children:'#include <disp3D_rhi/view/brainview.h>\n#include <disp3D_rhi/model/braintreemodel.h>\n#include <fs/surface.h>\n#include <fs/annotation.h>\n\n// Create view and model\nBrainView *view = new BrainView();\nBrainTreeModel *model = new BrainTreeModel();\nview->setModel(model);\n\n// Load FreeSurfer surfaces\nFSLIB::Surface surfLh("sample", 0, "inflated", subjectsDir);\nFSLIB::Surface surfRh("sample", 1, "inflated", subjectsDir);\nmodel->addSurface("sample", "lh", "inflated", surfLh);\nmodel->addSurface("sample", "rh", "inflated", surfRh);\n\n// Load annotations\nFSLIB::Annotation annotLh("sample", 0, "aparc", subjectsDir);\nmodel->addAnnotation("sample", "lh", annotLh);\n\nview->show();\n'})}),"\n",(0,s.jsx)(r.h3,{id:"adding-a-connectivity-network",children:"Adding a Connectivity Network"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-cpp",children:'#include <disp3D_rhi/model/braintreemodel.h>\n#include <connectivity/network/network.h>\n\n// After computing connectivity (see Connectivity library docs)\nQList<Network> networks = Connectivity::calculate(settings);\nmodel->addNetwork(networks.first(), "PLI");\n'})}),"\n",(0,s.jsx)(r.h2,{id:"shader-modes-and-visualization-modes",children:"Shader Modes and Visualization Modes"}),"\n",(0,s.jsxs)(r.p,{children:["The renderer supports multiple shader pipelines (",(0,s.jsx)(r.code,{children:"ShaderMode"}),") and surface overlay modes (",(0,s.jsx)(r.code,{children:"VisualizationMode"}),"):"]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"ShaderMode"}),(0,s.jsx)(r.th,{children:"Effect"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"Standard"})}),(0,s.jsx)(r.td,{children:"Default Phong shading"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"Holographic"})}),(0,s.jsx)(r.td,{children:"Two-sided holographic effect"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"Anatomical"})}),(0,s.jsx)(r.td,{children:"Curvature-based coloring"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"Dipole"})}),(0,s.jsx)(r.td,{children:"Dipole cone rendering"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"XRay"})}),(0,s.jsx)(r.td,{children:"Semi-transparent x-ray effect"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ShowNormals"})}),(0,s.jsx)(r.td,{children:"Surface normals as color"})]})]})]}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"VisualizationMode"}),(0,s.jsx)(r.th,{children:"Vertex Color Source"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ModeSurface"})}),(0,s.jsx)(r.td,{children:"Curvature-derived gray tones"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ModeAnnotation"})}),(0,s.jsx)(r.td,{children:"Atlas / parcellation region colors"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ModeScientific"})}),(0,s.jsx)(r.td,{children:"Scientific colormap"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"ModeSourceEstimate"})}),(0,s.jsx)(r.td,{children:"STC amplitude overlay"})]})]})]})]})}function h(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},28453(e,r,n){n.d(r,{R:()=>d,x:()=>a});var i=n(96540);const s={},t=i.createContext(s);function d(e){const r=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:d(e.components),i.createElement(t.Provider,{value:r},e.children)}}}]);